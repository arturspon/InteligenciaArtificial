{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercicios_Aula7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM2lFqa58GeH2xq3sq8DvjI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arturspon/InteligenciaArtificial/blob/master/Exercicios_Aula7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm0uhaz4VwJk"
      },
      "source": [
        "### **Tarefa 1 - Exercício de rede simples**\n",
        "\n",
        "Abaixo, você usará o Numpy para calcular o output de uma rede simples com dois nós de input e um nó de output com uma função de ativação sigmóide. Para isso, será necessário:\n",
        "\n",
        "*   Implementar a função sigmóide.\n",
        "*   Calcular o output da rede.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm4cjb05WFQ5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_SjhIFnWvNu"
      },
      "source": [
        "inputs = np.array([0.7, -0.3])\n",
        "weights = np.array([0.1, 0.8])\n",
        "bias = -0.1\n",
        "\n",
        "output = sigmoid(np.dot(inputs, weights) + bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOIJ3rpyW19Y",
        "outputId": "ebdd76fd-30ca-48f5-edf7-f2c3c877fcec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('output: ')\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output: \n",
            "0.4329070950345457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yiVKlRQYXTl"
      },
      "source": [
        "### **Tarefa 2 - Gradiente de descida: o código**\n",
        "\n",
        "Agora, escrevendo o código levando em conta o caso de apenas uma unidade de saída e a função sigmóide como função de ativação f(h)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhmJhMFbY32Q"
      },
      "source": [
        "def sigmoid_prime(x):\n",
        "    # Derivada da função sigmoid\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjER5t0TZLtn"
      },
      "source": [
        "#Dados de input\n",
        "x = np.array([0.1, 0.3])\n",
        "#alvo\n",
        "y = 0.2\n",
        "#pesos\n",
        "weights = np.array([-0.8, 0.5])\n",
        "#taxa de aprendizado\n",
        "learnrate = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84VXijBpbKTA"
      },
      "source": [
        "#combinacao linear \n",
        "h = x[0] * weights[0] + x[1] * weights[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLxdUlqocCHG"
      },
      "source": [
        "#output da rede\n",
        "nn_output = sigmoid(h)\n",
        "\n",
        "#erro do output\n",
        "error = y - nn_output\n",
        "\n",
        "#gradiente\n",
        "output_grad = sigmoid_prime(h)\n",
        "\n",
        "#termo do erro\n",
        "error_term = error * output_grad\n",
        "\n",
        "#passo do gradiente\n",
        "del_w = learnrate * error_term * x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4U7ySJddfkP",
        "outputId": "fb760ac3-cceb-4411-fb54-e0b5474789ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Del_w:\", del_w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Del_w: [-0.0039638  -0.01189141]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWi9515CeGAE"
      },
      "source": [
        "### **Tarefa 3 - Exercício de programação**\n",
        "\n",
        "O objetivo é treinar a rede até que ela alcance um mínimo na média de erros quadrados (MEQ) dos dados de treinamento. Você deverá implementar:\n",
        "\n",
        "*   O output da rede: output.\n",
        "*   O erro do output: error.\n",
        "\n",
        "*   O termo do erro: error_term.\n",
        "*   Atualizar o passo: del_w +=.\n",
        "*   Atualizar os pesos: weights +=.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kd0RHOwh7BW",
        "outputId": "580d0005-0a90-4264-b207-76648d1a6fa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "n_features = x.shape[0] \n",
        "\n",
        "# Initialize \n",
        "np.random.seed(42)\n",
        "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
        "print(\"Weights\", weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights [0.28291879 0.9031849 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPWCYtbknYVw",
        "outputId": "2fcfe1ed-440c-43ba-c046-dfa7f2d1b854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "epochs=1000\n",
        "last_loss = None\n",
        "\n",
        "for e in range(epochs):\n",
        "  del_w = np.zeros(weights.shape)\n",
        "  h = np.dot(weights,x)\n",
        "  output = sigmoid(h)\n",
        "  error = y - output\n",
        "  error_term = error * sigmoid_prime(h)\n",
        "  del_w +=  learnrate * error_term * x\n",
        "  weights += del_w\n",
        "  \n",
        "  #  mean square error \n",
        "  if e % (epochs / 10) == 0:\n",
        "    out = sigmoid(np.dot(x, weights))\n",
        "    loss = np.mean((out - y) ** 2)\n",
        "    if last_loss and last_loss < loss:\n",
        "        print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "    else:\n",
        "        print(\"Train loss: \", loss)\n",
        "    last_loss = loss\n",
        "print(\"##########################################\")\n",
        "print(\"output:\", output)\n",
        "print(\"error:\", error)\n",
        "print(\"error_term\", error_term)\n",
        "print(\"del_w:\",del_w)\n",
        "print(\"weights\",weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train loss:  2.3278795440867036e-09\n",
            "Train loss:  1.8016568592123533e-09\n",
            "Train loss:  1.3944026618872038e-09\n",
            "Train loss:  1.0792158823379476e-09\n",
            "Train loss:  8.352797885239192e-10\n",
            "Train loss:  6.464853892111341e-10\n",
            "Train loss:  5.003664470277317e-10\n",
            "Train loss:  3.872755163128927e-10\n",
            "Train loss:  2.997464249183674e-10\n",
            "Train loss:  2.3200098946543183e-10\n",
            "##########################################\n",
            "output: 0.20001343464716814\n",
            "error: -1.3434647168131253e-05\n",
            "error_term -2.1496518383229037e-06\n",
            "del_w: [-1.07482592e-07 -3.22447776e-07]\n",
            "weights [-1.40253906 -4.15318866]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_-XHnintfnd"
      },
      "source": [
        "Tarefa 4  - MPL 4x3x2\n",
        "A seguir, você implementará uma rede 4x3x2 orientada a frente, com funções de ativação sigmóide em ambas as camadas.\n",
        "Coisas a fazer: \n",
        "\n",
        "*   Calcular o input da camada oculta.\n",
        "*   Calcular o output da camada oculta.\n",
        "*   Calcular o input da camada de output.\n",
        "*   Calcular o output da rede."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWdC4J8cuk8l",
        "outputId": "4b926a04-4801-48d5-9ff3-49b103eb2533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# rede 4x3x2\n",
        "N_input = 4\n",
        "N_hidden = 3\n",
        "N_output = 2\n",
        "\n",
        "\n",
        "# inputs\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(4)\n",
        "\n",
        "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
        "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
        "\n",
        "hidden_layer_in = np.dot(X,weights_input_to_hidden)\n",
        "hidden_layer_out = sigmoid(hidden_layer_in)\n",
        "\n",
        "print('Hidden-layer In:',hidden_layer_in)\n",
        "print('Hidden-layer Output:',hidden_layer_out)\n",
        "\n",
        "output_layer_in = np.dot(hidden_layer_out,weights_hidden_to_output)\n",
        "output_layer_out = sigmoid(output_layer_in)\n",
        "\n",
        "print('Output-layer In:', output_layer_in)\n",
        "print('Output-layer Output:', output_layer_out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hidden-layer In: [-0.34365494 -0.29801368  0.00097362]\n",
            "Hidden-layer Output: [0.41492192 0.42604313 0.5002434 ]\n",
            "Output-layer In: [-0.00739221 -0.05842573]\n",
            "Output-layer Output: [0.49815196 0.48539772]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hEGGuCCwnDD"
      },
      "source": [
        "Tarefa 5 - Backpropagation\n",
        "Exercício de Backpropagation\n",
        "Abaixo, você implementará o código para calcular uma rodada de atualização com backpropagation para dois conjuntos de pesos. \n",
        "Escrevi o andamento para frente, o seu objetivo é escrever o andamento para trás.Coisas a fazer●Calcular o erro da rede.●Calcular o gradiente de erro da camada de output.●Usar a backpropagation para calcular o erro da camada oculta.●Calcular o passo de atualização dos pesos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW3pFDeWyhW3",
        "outputId": "dee1fbe0-1da0-4d0b-eed0-41d5d9ecf313",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = np.array([0.5, 0.1, -0.2])\n",
        "target = 0.6\n",
        "learnrate = 0.5\n",
        "\n",
        "weights_input_hidden = np.array([[0.5, -0.6],\n",
        "                                 [0.1, -0.2],\n",
        "                                 [0.1, 0.7]])\n",
        "\n",
        "weights_hidden_output = np.array([0.1, -0.3])\n",
        "\n",
        "## Forward pass\n",
        "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
        "hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
        "output = sigmoid(output_layer_in)\n",
        "\n",
        "## Backwards pass\n",
        "## TODO: Calculate output error\n",
        "error = target - output\n",
        "\n",
        "# TODO: Calculate error term for output layer /  da para usar sigmoid prime\n",
        "output_error_term = error * output * (1 - output)\n",
        "\n",
        "# TODO: Calculate error term for hidden layer\n",
        "hidden_error_term = output_error_term * weights_hidden_output * hidden_layer_output * (1- hidden_layer_output)\n",
        "\n",
        "\n",
        "# TODO: Calculate change in weights for hidden layer to output layer\n",
        "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
        "\n",
        "# TODO: Calculate change in weights for input layer to hidden layer\n",
        "delta_w_i_h = learnrate * hidden_error_term * x[:,None]\n",
        "\n",
        "print('Change in weights for hidden layer to output layer:')\n",
        "print(delta_w_h_o)\n",
        "print('Change in weights for input layer to hidden layer:')\n",
        "print(delta_w_i_h)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Change in weights for hidden layer to output layer:\n",
            "[0.00804047 0.00555918]\n",
            "Change in weights for input layer to hidden layer:\n",
            "[[ 1.77005547e-04 -5.11178506e-04]\n",
            " [ 3.54011093e-05 -1.02235701e-04]\n",
            " [-7.08022187e-05  2.04471402e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bot1qe3SyqUm"
      },
      "source": [
        "Tarefa 6 - Backpropagation_2Implementando o BackpropagationAgora você implementará o algoritmo de backpropagation em uma rede treinada com os dados de admissão em universidades. Você possui tudo o que precisa para resolver esse exercício nos exercícios anteriores.Seus objetivos aqui:●Implementar o andamento adiante.●Implementar o algoritmo de backpropagation.●Atualizar os pesos."
      ]
    }
  ]
}